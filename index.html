<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Web Media Application Developer Guidelines</title>
    <script src="https://www.w3.org/Tools/respec/respec-w3c-common" class="remove"></script>
    <script class="remove">
      var respecConfig = {
        specStatus: "CG-DRAFT",
        editors: [{
          name: "Jeff Burtoft",
          url: "mailto:jeffburt@microsoft.com",
          company: "Microsoft",
          companyURL: "http://www.microsoft.com"
        },{
          name: "Thasso Griebel",
          url: "mailto:thasso.griebel@castlabs.com",
          company: "CastLabs",
          companyURL: "http://www.castlabs.com"
        },{
          name: "Joel Korpi",
          url: "mailto:jkorpi@jwplayer.com",
          company: "JW Player",
          companyURL: "http://www.jwplayer.com"
        }],
        processVersion: 2015,
        edDraftURI: "http://w3c.github.io/webmediaguidelines",
        shortName: "dahut",
        wg: "Web Media API Community Group",
        wgURI: "https://www.w3.org/community/webmediaapi/",
      };
    </script>
  </head>
  <body>
    <section id="abstract">
      <p> This specification is a companion guide to the <a
          href="https://w3c.github.io/webmediaapi/" target="_blank">Web
          Media API spec</a>. While the Web Media API spec is targeted
        at device implementations to support media web apps in 2017,
        this specification will outline best practices and developer
        guidance for implementing web media apps. This specification
        should be updated at least annually to keep pace with the
        evolving Web platform. The target devices will include any
        device that runs a modern HTML user agent, including
        televisions, game machines, set-top boxes, mobile devices and
        personal computers. </p>
      <p> The goal of this Web Media API Community Group specification
        is to transition to the W3C Recommendation Track for standards
        development. </p>
    </section>
    <section id="sotd"></section>
    <section>
      <h2>Introduction</h2>
      <ol class="ednote" title="Notes on v1 draft specification:">
        <li>This document is directed towards application developers.
          Its content will contain best practices for building media
          applications across devices, it will not provide direction to device
          manufactures or User Agent implementers. </li>
        <li>This is a companion spec put forth by the Web Media API
          Community Group.</li>
      </ol>
      <section>
      <h3>Scope</h3>
      <p>The scope of this document includes general guidelines, best
        practices, and examples for building media applications across
        web browsers and devices. </p>
      <p>The target audience for these guidelines are software
        developers and engineers focused on building cross-platform,
        cross-device, HTML5-based applications that contain
        media-specific use-cases.</p>
      <p>The focus of this document is on HTML5-based applications,
        however the use-cases and principles described in the guidelines
        can be applied to native applications (applications that has
        been developed for use on a particular platform or device). The
        examples in this document provides a starting point to build
        your media application and includes example implementations from
        various providers and vendors. This document also includes
        sample content and manifests as well as encoding guidelines to
        maximize the provide hints on achieving the best quality and
        efficiency for your media applications.</p>
      </section>
      <section>
      <h3>Accessibility</h3>
      <p>These guidelines will cover making your applications compliant
        with accessibility requirements from the perspective of
        delivering and consuming media. However, to make sure your
        entire application is accessibility-friendly, please see the <a
          href="https://www.w3.org/WAI/intro/wcag">W3C Web Content
          Accessibility Guidelines</a>.</p>
      </section>
      <section>
      <h3>Glossary of Terms</h3>
      <p>In order to provide a common language to build this document
        and communicate concepts to the end reader, we are providing a
        Glossary of Terms to this guideline document.<br>
      </p>
      <p>
        <table border="1" cellspacing="0" cellpadding="4">
          <colgroup><col width="181"><col width="578"></colgroup><tbody>
            <tr>
              <td><b>Term</b></td>
              <td><b>Definition</b></td>
            </tr>
            <tr>
              <td>360 Video</td>
              <td>Video content where a view in every direction is
                recorded at the same time, shot using an
                omni-directional camera or a collection of cameras.
                During playback the viewer has control of the viewing
                direction like a spherical panorama.</td>
            </tr>
            <tr>
              <td valign="top">AAC<br>
              </td>
              <td valign="top">Advanced Audio Coding is a proprietary
                audio coding standard for lossy digital audio
                compression. Designed to be the successor of the MP3
                format, AAC generally achieves better sound quality than
                MP3 at the same bit rate.<br>
              </td>
            </tr>
            <tr>
              <td>Adaptive Streaming (ABR)<br>
              </td>
              <td>Adaptive streaming / adaptive bitrate (also known as
                ABR) is a process that adjusts the quality of a video
                delivered to a web page based on changing network
                conditions to ensure the best possible viewer
                experience. The most common ABR technologies are HTTP
                Live Streaming (HLS) created by Apple and Dynamic
                Adaptive Streaming over HTTP (DASH).<br>
              </td>
            </tr>
            <tr>
              <td>AVOD</td>
              <td>Advertising-supported Video on Demand. AVOD services
                monetize their content by serving ads to users, as
                opposed to other business models such as paid
                subscription or pay-per-title.</td>
            </tr>
            <tr>
              <td valign="top">Bit rate<br>
              </td>
              <td valign="top"> Bit rate, (also known as data rate), is
                the amount of data used for each second of video. In the
                world of video, this is generally measured in kilobits
                per second (kbps), and can be constant and variable. </td>
            </tr>
            <tr>
              <td valign="top">Codec<br>
              </td>
              <td valign="top">A codec is the algorithm used to capture
                analog video or audio in digital form. Most codecs
                employ proprietary coding algorithms for data
                compression. MP3, H.264, and HEVC are examples of
                codecs.<br>
              </td>
            </tr>
            <tr>
              <td>Content Delivery Network (CDN)</td>
              <td>A content delivery network (CDN) is a system of
                distributed servers (network) that deliver pages and
                other media content to a user, based on the geographic
                locations of the user, the origin of the webpage and the
                content delivery server.</td>
            </tr>
            <tr>
              <td>Chunk(ing)</td>
              <td>ABR technologies typically break a video or audio
                stream into chunks to make transmission more compact and
                efficient. These chunks are typically 2-10 seconds in
                length and contain at least one I-Frame so that the
                video player has complete information for which to
                render the video from that point in the manifest. </td>
            </tr>
            <tr>
              <td>Closed Captions</td>
              <td>Used for accessibility and usability, closed captions
                are a visual representation of the audio content of a
                media file stored as metadata file or track and
                displayed as an overlay in the video player in
                synchronization with the video/audio track. Typical
                formats for closed captioning are WebVTT and SRT.<br>
              </td>
            </tr>
            <tr>
              <td>DRM</td>
              <td>A combination of encryption, authentication and access
                control technologies that are used to prevent
                unauthorized users from consuming copyrighted media
                content. The most widely used DRM solutions today are
                Microsoft PlayReady, Google Widevine, and Apple FairPlay
                Streaming.<br>
                <br>
                There are also open technologies that use standards such
                as Common Encryption (MPEG-CENC) and Media Source
                Extensions (MSE) to create a secure environment without
                third-party products.<br>
              </td>
            </tr>
            <tr>
              <td>Embeds</td>
              <td>Most video players in a web page use what's called an
                "Embed" or "Embed Code" that is placed in the code of
                your HTML page or app that will render the video
                playback environment. </td>
            </tr>
            <tr>
              <td>Encoding</td>
              <td>(see Transcoding)<br>
              </td>
            </tr>
            <tr>
              <td>Encrypted Media Extensions<br>
              </td>
              <td>Encrypted Media Extensions (EME) is a proposed W3C
                specification for providing a communication channel
                between web browsers and digital rights management (DRM)
                agent software. This allows the use of HTML5 video to
                play back DRM-wrapped content such as streaming video
                services without the need for third-party media plugins
                like Adobe Flash or Microsoft Silverlight. The use of a
                third-party key management system may be required,
                depending on whether the publisher chooses to scramble
                the keys.<br>
              </td>
            </tr>
            <tr>
              <td valign="top">Format<br>
              </td>
              <td valign="top">A format or "container format", is used
                to bind together video and audio information, along with
                other information such as metadata or even subtitles.
                For example, .MP4, .MOV, .WMV etc. are all container
                formats that contain both audio, video, and metadata in
                a single file. Formats contain tracks that are encoded
                using codecs. For example an .MP4 might use the AAC
                audio codec together with the H.264 video codec.
                <meta charset="utf-8">
              </td>
            </tr>
            <tr>
              <td>H.264</td>
              <td>Also known as MPEG-4 AVC (Advanced Video Coding) it is
                now one of the most commonly used recording formats for
                high definition video. It offers significantly greater
                compression than previous formats. </td>
            </tr>
            <tr>
              <td valign="top">HEVC<br>
              </td>
              <td valign="top">High Efficiency Video Coding is one of
                the newest generation video codecs that is able to
                achieve efficiency up to 4x greater than H.264, but it
                requires the HEVC codec to be present on the device.
                HEVC typically takes considerable more processing power
                to encode and decode than older codecs such as H.264.<br>
              </td>
            </tr>
            <tr>
              <td>HLS</td>
              <td>HTTP Live Streaming (HLS) is an adaptive streaming
                technology created by Apple that allows the player to
                automatically adjust the quality of a video delivered to
                a web page based on changing network conditions to
                ensure the best possible viewer experience. </td>
            </tr>
            <tr>
              <td>I-Frame (video)</td>
              <td>In video compression, an I-Frame is an independent
                frame that is not dependent on any future or previous
                frames to present a complete picture. I-Frames are
                necessary to provide full key frames for the
                encoder/decoder. </td>
            </tr>
            <tr>
              <td>Live Streaming</td>
              <td>Live streaming is a type of broadcast that is
                delivered over the Internet where the source content is
                typically a live event such as a sporting event,
                religious service, etc. Unlike VOD, viewers of a live
                stream all watch the same broadcast at the same time. </td>
            </tr>
            <tr>
              <td>manifest </td>
              <td>A manifest is a playlist file for adaptive streaming
                technologies (such as HLS, DASH, etc.) that provides the
                metadata information for where to locate each segment of
                a video or audio source. Depending on the configuration,
                some technologies have two types of manifests: one
                "master" manifest that contains the location of each
                rendition manifest and one rendition manifest for each
                rendition that contains the location (relative or
                absolute) of each chunk of a video or audio source. </td>
            </tr>
            <tr>
              <td>Metadata Track</td>
              <td>ABR streaming technologies contain the ability to
                include not only video and audio tracks within the
                stream, but also allow for metadata tracks for
                applications such as closed captions, advertising cues,
                etc.<br>
              </td>
            </tr>
            <tr>
              <td>MPEG-Dash</td>
              <td>Dynamic Adaptive Streaming over HTTP (DASH), also
                known as MPEG-DASH, is an adaptive bitrate streaming
                technique that enables high quality streaming of media
                content over the Internet delivered from conventional
                HTTP web servers. </td>
            </tr>
            <tr>
              <td>Player<br>
              </td>
              <td>A video or audio media player that is used to render a
                media stream in your application environment. There are
                commercially available and open source video players and
                SDKs for almost every platform. </td>
            </tr>
            <tr>
              <td>Rendition </td>
              <td>A specific video and audio stream for a target quality
                level or bitrate in a adaptive streaming set or
                manifest. For example, most HLS or DASH adaptive
                streaming manifests contain multiple renditions for
                different quality/bitrate targets so that the viewer
                automatically views the best quality content for their
                specific Internet connection speed. </td>
            </tr>
            <tr>
              <td>SDK</td>
              <td>A Software Development Kit is a set of tools that
                allow the creation of applications for a certain
                framework or development platform. Typically they are
                the implementation of one or more APIs to interface to a
                particular programming language, and include debugging
                utilities, sample code, and documentation. </td>
            </tr>
            <tr>
              <td>Self-hosted player</td>
              <td>The JavaSript application files are hosted from your
                own domain. Use this method if you have your own CDN or
                want to stay locked into a version of the player. Note:
                we do not recommend this as the player updates every 4-6
                weeks.</td>
            </tr>
            <tr>
              <td>Streaming</td>
              <td>Streaming of media content is typically the delivery
                of media assets over Internet protocols such as HTTP or
                RTMP. </td>
            </tr>
            <tr>
              <td>SVOD</td>
              <td>Subscription-supported Video on Demand. SVOD services
                monetize their content through paid subscriptions from
                users, as opposed to other business models such as
                advertising or pay-per-title.</td>
            </tr>
            <tr>
              <td>Transcoding</td>
              <td>Also referred to as compression, this is the process
                of removing redundancies from raw video and audio data,
                thereby reducing the amount of data required to deliver
                the media across a network, on a physical disc, etc. The
                process can result in reduced visual or auditory quality
                of the encoded media, but the loss is usually
                imperceptible to the user. H.264 and HEVC are examples
                of codecs that use compression during transcoding. </td>
            </tr>
            <tr>
            	<td>Trick Play/Mode</td>
            	<td>Trick mode, sometimes called trick play, is a feature of digital video systems including Digital Video Recorders and Video on Demand systems that mimics the visual feedback given during fast-forward and rewind operations that were provided by analogue systems such as VCRs.</td>
            </tr>
            <tr>
              <td>URL Signing</td>
              <td>URL signing is a mechanism for securing access to
                content. When URL Signing enforcement is enabled on a
                property; requests to a content server or content
                delivery network must be signed using a secure token.
                The signing also includes an expiration time after which
                the link will no longer work. This ensures that only
                links generated by the customer can be used to access
                their content and that if those links are used
                elsewhere, the use will expire at the expiration time.</td>
            </tr>
            <tr>
              <td>VOD</td>
              <td>Video on demand is video that is served at the request
                of a user. Delivery is typically through a content
                delivery network to optimize delivery speed and
                efficiency.<br>
              </td>
            </tr>
            <tr>
              <td>VR</td>
              <td>Virtual Reality is a realistic, immerse,
                three-dimensional environment, created using interactive
                software and hardware, and experienced or controlled by
                movement of the body. VR experiences typically require
                wearing a head mounted display (HMD).</td>
            </tr>
          </tbody>
        </table>
      </p>
      <p>For a more detailed list of relevant terms, please see the
        glossary of terms for the vendors or technologies you use in
        your workflow. </p>

      </section>
    </section>
    <section>
      <h2>Media Playback Use Cases</h2>
   

        <section>
        <h3>General Description</h3>
        <p>Material (typically in video or audio content) is made
          available by a content provider via a web-enabled application
          and delivered by a content distribution network. There are
          three distinct interlocking processes: generation, delivery
          and consumption / playback. </p>
        </section>
        <section>
        <h3>Content Generation</h3>
        <p> Original Content (mezzanine files, discussed in section below) is normally delivered to the service
          provider as a file with near lossless compression.
          Specifically, for High Definition content at 1080p with 50 or
          60 frames-per-second, the bitrate of the original content is
          typically 25Mbps to 150Mbps. </p>
        <p> Streaming content is generated by encoding the source
          against an encoding profile. Firstly, the content is
          duplicated into different versions of the file that target
          delivery over a connection that achieves a certain bandwidth,
          these are the different bitrates. After this is it split into
          segments of specified length. </p>
        <p> The first process is defined by an encoding profile. A
          profile describes the set of constraints to be used when video
          is being prepared for consumption by a range of video
          applications. The description includes the different bitrates
          to be generated during the encoding process that will allow
          for the same content to be consumed on a wide variety of
          devices on different networks from cellular to LAN. </p>
        <p> The second process is performed by a packager which segments
          the different bitrates. These are then packaged into a
          transport format such as transport streams (.ts) or fragmented
          MP4s (.m4s) after this they are encrypted with a DRM that is
          suitable for the environment where the content is going to be
          played out. NOTE: DRM is optional in this process. The packager is also responsible for the
          generation of a manifest file, typically a DASH (.mpd), HLS
          (.m3u8) or possibly Smooth (.ism) or HDS (.f4v), that
          specifies the location of the media and its format. </p>
        </section>
        <section>
        <h3>Content Delivery</h3>
        <p> After the content has been generated the resulting segments
          of video and corresponding manifest files are pushed to an
          origin server. However, the assets are rarely delivered
          directly from the origin. </p>
        <p> At this stage, the control in the chain switches to the
          client's web video application. The content provider supplies
          the client with the URL of a manifest file located on a CDN
          rather than the origin. The manifest is typically passed to
          a player. The player makes a GET request for the manifest from
          CDN edge, its location is determined by DNS. The CDN does one
          of two things: if it has the asset it returns it to the
          player, if it does not it requests it from the origin. When it
          receives it the CDN caches the manifest for use by other
          sessions and then then returns it to the requesting client. </p>
        </section>
        <section>
        <h3>Content Playback</h3>
        <p> The player parses the manifest. At this point the behavior
          differs between players found on the different devices
          depending on the transport formats. However, broadly, it
          behaves in the following way: <br>
        </p>
        <ol>
          <li>The client uses DRM license URL to request a secure key to
            enable decoding of the media.</li>
          <li>The player's ABR (adaptive bitrate) algorithm determines
            the bandwidth available to the client by examining the
            response times associated with the request for the first
            segment of video, how many bytes were received over what
            time period. This provides enough information to determine
            the playback quality that the player can sustain over a
            proportion of the length of the asset, or in the case of
            live streaming a specific timeframe. </li>
          <li>Once the player has this information it can then compare
            this with the metadata from the manifest that describes the
            different qualities that the content provider is supplying.
            It picks the quality level with an average bitrate that is
            as close to the available bitrate but within its bounds to
            avoid a situation where a consistent experience is
            interrupted as the player requires more data than the
            current network bandwidth can supply, where the player’s
            buffer is emptying faster than it is being filled. </li>
          <li>It then requests a segment from a location on the edge
            server that typically relative to the location of the
            manifest. </li>
          <li>Once it has received the segment it is then typically
            decrypted in accordance with the specific DRM used. </li>
          <li>It then adds it to the player's video buffer. </li>
          <li>The media engine pulls the video data from the buffer and
            passes it to the video surface where it is rendered. </li>
          <li>If the available bandwidth remains constant the player
            will continue to request segments from the same bitrate
            stream, pulling down chunks and filling its video buffer. In
            the event a change in network availability the player will
            make a decision about the need to either drop to a lower
            bitrate stream or request a higher bitrate child manifest
            and associated segment. </li>
        </ol>
        </section>
      	<section>
        <h3>On-Demand Streaming (VOD)</h3>
        <p> Despite the almost identical mechanics used for the two
          types of content, VOD and linear, the contents generation,
          delivery and playout a large organization will typically
          maintain two distinct workflows as there are subtle but
          important ways in which they differ. </p>
        </section>
        <section>
        <h3>Content Generation</h3>
        <p> For VOD the source is typically static file-based rather than a feed. The
          encoding profiles are also subtly different. A greater
          priority can be placed on high quality as the latency, time to
          live, is not a requirement. To this end the encoder is able to
          prioritize density over quality via configuration allowing VOD
          encoders to spend more time on each frame. There are important
          differences in the manifests created. In HLS there is a tag
          that tells the player whether the playlist is describing on
          demand material: #EXT-X-PLAYLIST-TYPE:VOD. As we will see
          shortly this is used by the player. There are also client side
          restrictions where certain profiles are blocked due to rights
          restrictions and network consumption capped bitrates on movies
          and entertainment whilst being allowed on sports content.
          Fragments size will also effect playout as a player's ABR can
          be more responsive if the chunks are smaller, e.g. 2 seconds
          rather than 10 seconds. </p>
        </section>
        <section>
        <h3>Content Delivery</h3>
        <p> The CDN configuration and topology for delivering VOD
          content is also different to linear. There are different
          levels of caching; popular VOD content which is kept closer to
          the edge in the CDN network in this way it can be delivered to
          customers faster than an edge server that isn't tuned for high
          volume delivery. Older and less popular content is retained in
          mid-tier caching whilst the long tail content is relegated to
          a lower tier. </p>
        </section>
        <section>
        <h3>Content Playback</h3>
        <p> As mentioned in the content generation section the player
          uses a tag within the manifest to determine the playout type.
          In HLS if there is a type with the value of VOD, then the player will not
          reload the manifest. This has important consequences if there
          are changes in availability after the session as commenced. In
          DASH the difference between a live and VOD playlist are more
          subtle (more detail) </p>
        <p> There are other differences in playout as well. Unlike
          linear, a VOD asset has a predefined duration, information
          around duration and current time can be used to update the UI
          to provide feedback to the user on the amount and proportion
          of the asset watched. </p>
        <p> At a broader level the UX requirements will be different in
          respect to the need for representing static rather than linear
          content where a tile view rather an EPG (electronic program
          guide) is required. There is also a requirement for Trick
          Play. </p>
        </section>
       <section>
        <h3>VOD Use-cases</h3>
        <p> In the previous section, we outlined the use cases
          associated with video streaming. In this section, we give some
          examples of use-cases that are specific to on-demand streaming
          and are mainly related to strategies employed on the clients
          to improve performance in some way. </p>
        <section>
        <h4> Pre-caching </h4>
        <p> The key performance indicator for most streaming services
          will be the percentage of sessions that experience buffering
          as a ratio to the length of the session. Buffering, the state
          of the video application when the player has insufficient
          content within its framebuffer to continuously play content,
          within a session has a direct relationship to engagement and
          as a consequence retention. For every second of buffering
          within a session 10% of users abandon a video stream.
          Pre-caching is a strategy used in on-demand streaming. Web
          video application developers will use points within an
          application's UX to pre-cache content, for example when
          entering a mezzanine/synopsis page the application might
          connect to a stream and begin to pull content and either add
          it directly to the player’s video buffer or alternatively
          store the chunks locally. The consequence of this is that when
          or if the user chooses to play the content after reading the
          synopsis the video will commence playing without buffering and
          hence provide the user with a preferable experience to a
          buffering indicator. This technique is used by Netflix, Sky
          and the BBC in the case of on-demand content being watched in
          an in-home context. This technique is not used for cellular
          sessions where user’s mobile data would be consumed
          potentially on content that they do not watch. </p>
        </section>
      </section>
        <section>
        <h3>VOD – further considerations</h3>
      	<section>
        <h4>Byte range requests in context of web video application</h4>

        <p>
        An HTTP range request allows a client to request a portion of a larger
        file. This is helpful in a few scenarios. In the vod scenario the user
        may want to access a specific location in a file, a range request
        allows you to access content at a specific location without downloading
        the entire transport chunk.
        </p>
        <p>
        Both the player and the server need to support range requests. The
        player needs to be able to playback a source buffer and server must be
        configured to serve ranges. The exchange begins when a client makes an
        HTTP HEAD request, the server will then respond with a header that
        includes Accept-Ranges: bytes. If this is the case then the client can
        issue subsequent requests for partial content. The returned bytes are
        added to an Array Buffer and then appended to the sourceBuffer which in
        turn is used as the SRC parameter of the of the video HTML5 tag/element.
        </p>
        </section>
        <section>
        <h4>HDCP security requirement for HDMI</h4>
        <p>
        Some content have limitations placed on its distribution by the
        owners. There are different ways of protecting the owner’s rights. The
        most common is DRM (digital rights management) which prevents the
        content being watched on a client device if the user cannot ‘unlock’ it
        with a key. However, once the content is unlocked the service provider
        should make every effort to prevent the user from re-distributing this
        content.
        </p>
        <p>
        A technical solution to prevent interception as a signal travels from a device
        to a television or projector is termed High Bandwidth Content Protection
        (HDCP). If implemented by a device manufacturer it takes the form of a key
        exchange between devices. If the exchange fails the signal is stopped and the
        client is notified. The client can then use this notification to present the
        user with a message altering them.
        </p>
        </section>
        <section>
        <h4>Watermarking</h4>
        <p>
        Support for real-time watermarking is becoming an important consideration
        for distributors of high-value content. A service provider’s right to
        distribute content is linked to their ability to protect it with studios and
        sports channels insisting on the capacity for a service provider to detect a
        breach of copyright on a stream by stream basis.  A service provider can
        include a vendors SDK in the client that can add a watermark at run-time.
        Normally, the watermark is invisible to the consumer and takes the form of a
        digital signal, this is referred to as forensic water marking. Part of the
        service provided by the vendor of this technology if to monitor the black
        market for re-distributed material. Illicit streams or files are intercepted
        screened for the digital fingerprint inserted on the client. If a suspect
        stream is found they then direct the service providers to the location where
        the streams have been misappropriated.
        </p>

        <p>
        Whilst this is not an issue that developers will often be faced with,
        this requirement can dictate the choice of platform for the content
        distribution. This type of process in itself requires processing power
        and platforms such as Android, iOS or Roku provide developers with an
        interface to low-level functionality in situations where the resources
        can be effectively ring-fenced. The options for watermarking in the
        browser are limited to overlays and this limitation is then reflected
        in a distributor’s choice of platform for their web video application.
        </p>
        </section>
        </section>
        <section>
        <h3>Live Streaming</h3>
            <p>
              Even though Live and On-Demand streaming scenarios have a lot in
              common, there are a few distinct differences.
            </p>
            <section>
            <h4>Content Generation</h4>
            <p>
            In contrast to VoD content generation, the typical input is a live
            feed of data. Usually there is also a higher priority on
            low-latency and time to live, which is in turn reflected in smaller
            segments.
            </p>
            <p>
            A big difference between VoD and Live content generation can be
            found in the differences between manifests for the two content
            types.
            Besides a general profile that tells the player if the manifest
            represents live content, there are a few other, important properties
            that define how a player will be able to playback the live content,
            when updates will be fetched, and how close to the live edge playback
            starts.
            These properties are pre-defined and expressed in the manifest during
            content preparation, but play an important role in content playback
            and view experience.
            </p>
            </section>
            <section>
            <h4>Content Delivery</h4>
            <p>
            The content delivered through a CDN is generally very similar to the
            VoD playback use-case. There might be slight differences when it
            comes to caching and distribution of segments, and you might
            observe higher load on the origin when playback is configured to be
            very close to the live edge.
            </p>
            </section>
            <section>
            <h4>Content Playback</h4>
            <p>
            Even though playback and playback parameters between Live and VoD
            playback are very similar, there are critical differences. Maybe
            the biggest difference is the playback start position. While VoD
            playback session usually start at the beginning on the stream, a
            Live playout usually starts close to the live edge of the stream,
            with only a few segments between the playback start position and
            the end of the current manifest.
            </p>
            <p>
            The other main difference is that the live content changes over
            time and the player needs to be aware of these changes. In both
            DASH and HLS, this results in regular manifest updates. Depending
            on the format, it is possible to define the interval for the
            manifest updates.
            </p>
            <p>
            What happens during a typical playback session is:
            <ol>
                <li>Player loads the manifest for the first time</li>
                <li>Playback starts at the live-edge defined in the manifest</li>
                <li>Player re-loads the manifest in regular, pre-defined, intervals.</li>
                <li>The new manifest is used to update playback state information and the segment list</li>
            </ol>
            This is a typical loop that the player goes through on each
            manifest update cycle.
            </p>
            </section>
            <section>
            <h4>Potential Issues</h4>
            <p>
            One of the most common cases that an application needs to be
            prepared for is the player falling behind the live window. Assume
            for example that you have a buffer window of 10 seconds on your
            live stream. Even if the user interface does not allow explicit
            seeking, buffering events can still stall the playout and push the
            playback position towards the border of the live window and even
            beyond it. Depending on the player implementation the actual error
            might be different, but implementation should be aware of the
            scenario and be prepared to recover from it.
            </p>
            </section>
      </section>
      <section>
          <h3>Thumbnail Navigation</h3>
          <p>
          Thumbnails are are small images of the content taken at regular time
          intervals. They are an effective way to visualize scrubbing and
          seeking through content.
          </p>
          <p>
          There is currently no default way to add thumbnail support to a playback
          application. And there is not out-of-the-box browser support. However,
          since thumbnails are just image data, the browser has all the capabilities
          for a client to implement thumbnail navigation in an application.
          </p>
          <p>
          The most common way to generate thumbnails is to render a set of images out of
          the main content in a regular time interval, for example every 10 seconds.
          The information about the location of these images then needs to be passed
          down to the client, which can then request and load an image for a given
          playback position.  For more efficient loading, images
          are often merged into larger grids. This way, the client
          only needs to make a single request to load a set of thumbnails
          instead of a request per image.
          </p>
          <p>
          Unfortunately, neither DASH or HLS do currently specify a way to
          reference thumbnail images directly from manifests. This however is
          certainly possible and there is already a <a
          href="http://dashif.org/wp-content/uploads/2017/03/CR-image-thumbnails-v0.9.pdf">proposal</a>
          to add support for thumbnails to the DASH-IF guidlines [[DASHIFIOP]].
          This would certainly simplify live playback case where the
          application has to update the information about thumbnails with each
          manifest update.
          </p>
      </section>
    </section>
    </section>
    <section>
      <h2>Media Playback Methods</h2>
      <section>
        <h3>Device Identification</h3>
        <p>Device identification is required both at the level of device
          type and family and also to uniquely identify a device.
          Different techniques are used in different environments.</p>
        <section>
        <h4>Device Type</h4>
        <p>In the context of a mobile client the broad device type is
          already known - you can’t install an Android client on an iOS
          device. However, operating systems evolve and are extended.
          Within the application layer you will still need to
          determine the level of support for feature X and branch your
          code accordingly.</p>
          <p>
          If the application is hosted or the same application is
          deployed via different app stores then the clients runtime
          could be more ambiguous. The classic approach is to use the
          navigator.userAgent. Whilst this returns a string that does
          not explicitly state the device by name a regular expression
          match can be used to look for patterns that can confirm the
          device family. As an example the name ‘Tizen’ can be found in
          the user agent strings for Samsung and ‘netcast’ for LG
          devices. In the Chrome browser the strings will be different
          on Windows, Mac, Android and webview.</p>
          <p>
          Another method similar to feature detection is to look for
          available APIs, many of these are unique to a device, for
          example if(tizenGetUser){ then do X }.</p>
        </section>
        <section>
        <h4>Unique device Identifier</h4>
        <p>After you have determined what type of device you are on you
          may need to identify that device uniquely. Most device
          manufacturers provide an API so once you have discovered what
          type of device you are on you then know what API’s will be
          available to you. Each manufacturer provides a string
          constructed in a different way, some use the serial number of
          the device itself whilst others use lower level unique
          identifiers taken from the hardware. In each case the
          application layer should attempt to namespace this in some way
          to avoid an unlikely, but theoretically possible, clash with
          another user in any server-side database of users and devices.<br>
          In a classic PC/browser, rather that mobile or STB (set-top-box),
          environment there are technical issues with using unique
          identifiers as a user can access these stored locally and
          either delete them or reuse them in another context, this
          could allow them to watch content on more devices than their
          user account allows. For this reason some vendors prefer to
          use DRM solutions that both create and encrypt unique
          identifiers in a way that obscures them from the user.</p>
         </section>
      </section>
      <section>
        <h3>Device Content Protection Capabilities</h3>
        <p>A web video application needs to determine the content
          protection capabilities of the device that is being used to
          playback the content. The method of doing this will vary from
          device to device and between DRM systems.
          </p>
          <p>
          Like most DRM Widevine is only implemented on a subset of
          available devices. For native, non-browser, playout in iOS
          Google provide Widevine CDM which acts as a wrapper for
          AVPlayer or other third party players. Beyond short form DASH
          video being transcoded by a third party plugin such as Viblast
          this presupposes HLS content encrypted with widevine.
          </p>

          <p>
          Regardless of a device's capability to play back a stream
          encrypted with a specific DRM it is worth noting that a
          content provider will be aware of this capability in advance
          and consequently encrypt the streams to target a specific
          device. Beyond technical feasibility: can device X play back
          content encrypted with DRM Y? the provider will have a number
          of considerations when choosing a DRM for playback on a
          specific device; cost, utility, complexity and content value
          (the last consideration being mapped to contractual
          obligations). As a consequence, in most situations the API
          call that requests the stream from a back end service will
          either be to a service that is only configured to return
          streams with a suitable encryption or the server will use data
          from the request headers or key-value pairs in the request
          payload to determine which streams to return.</p>

          <p>
          In the context of the browser the major vendors broadly
          dictate the DRMs available, Apple’s Safari browser and Apple
          TV support Apple’s FairPlay DRM. Microsoft's Internet Explorer
          and Edge browsers only support Microsoft's PlayReady out of
          the box whilst Google’s Widevine Modular is supported by
          Google’s Chrome browser but is also included the browser
          developers not tied to a significant hardware providers; Opera
          and Firefox.</p>
          <p>
          Browser DRM detection capabilities are tested via the EME
          (Encrypted Media Extensions) API. The API is an extension to
          the HTMLMediaElement. The process of determining the available
          system is broadly as follows: </p>
        <ol>
          <li>The stream is passed to the VIDEO or AUDIO HTML5 tag/media element</li>
          <li>The browser detects the stream is encrypted</li>
          <li>The media event encrypted is thrown</li>
          <li>A check is made to see if there are already MediaKeys
            associated with the element</li>
          <li>If there are no keys already associated with the element
            then use the EME API navigator.requestMediaKeySystemAccess()
            to determine which DRM system is available. This is done by
            passing a key value pair that includes a string
            representation of each MediaKey system to the above method
            which in turn returns a boolean</li>
          <li>Use the MediaKeySystemAccess.createMediaKeys() to return a
            new MediaKeys object.</li>
          <li>Then use HTMLMediaElement.setMediaKeys(mediaKeys)</li>
        </ol>

        <p>The remaining steps are covered in a later section on using EME
        but it’s worth noting at this stage that the
        navigator.requestMediaKeySystemAccess() is not uniformly
        implemented across all modern browsers that support EME. As an
        example Chrome returns true for ‘com.widevine.alpha’ however IE
        and Safari throw errors and Firefox returns null. A possible
        solution to this is offered <a href="https://stackoverflow.com/questions/35086625/determine-drm-system-supported-by-browser">here</a>
        (NB if this is determined to be relevant we can add more detail
        and/or include the code example directly).
        </p>
        </section>

      <section>
        <h3>Ad Insertions</h3>
        There are two primary ways to insert advertisements into a playback
        session. <i>Client-side ad insertion (CSAI)</i> and <i>Server-side
        ad insertion (SSAI)</i>. The difference here is that Server-side
        insertions are embedded into the playout directly, while Client-side
        insertions are added dynamically by the player and are handled in
        parallel to the main playout.

        <section>
        <h4>Client-side Ad Insertion</h4>
        <p>
        Client-side ad insertions typically involve using a script
        available in the client runtime, JavaScript in the case of the web,
        to insert an advert during the user’s session.
        </p>
        <p>
        As an example, an ad serving vendor provides a client script. Close
        to the player’s initiation the client library makes its API
        available. The web application listens to events associated with
        playback, for example the video elements media event ‘playing’. The
        web application then calls the DOM pause method on the video
        element and then calls the play method provided by the client-side
        ad library, passing it the ‘id’ of the video asset. This is then
        returned to the vendor, possibly along with other identifiers that
        can be used to target the audience with a specific ad. At this
        stage an auction is performed with business logic at the ad vendor
        determining which provider supplies the ad (this is a complex topic
        and outside the scope of this document). The vendor responds with a
        VAST (Video Ad Serving Template) payload that includes the URI of
        the ad content appropriate for the playback environment. In some
        cases, there is no ad, if this is the case the user is presented
        with the content they originally requested and control is passed
        back to the web video application. If there is an ad targeted
        against the content then the library performs DOM manipulation and
        injects a new video element into the document this is typically
        accompanied by a further script that provides the vendor with
        insights based on the current session. The ad plays. The ad object
        will conform to VPAID (Video Player Ad Serving Definition) and
        present a standardized interface to the player for possible
        interaction, it will issue a standard set of events which the web
        application can listen to. In response to an ‘adEnded’ event the
        local library will tear down the injected DOM elements and in turn
        issue an event that the web application can use to trigger a return
        to playing the original content.
        </p>
        <p>
        In the case of live linear CSAI web socket technology is
        often used. If the content has a dynamic nature, for example a
        sports event or a fashion show, a web socket connection is
        established and ads can then be ‘pushed’ when the editorial team
        thinks it’s appropriate. The web video application listens to the
        library that establishes and manages the connection. An event is
        pushed to the listening clients simultaneously, this event is then
        used to trigger the same set of events discussed above.
        </p>
        </section>

        <section>
        <h4>Server Side Ad Insertion</h4>
        <p>
        Especially in live playback environments, because of the live
        nature of the playout, ad-insertions are usually not handle client
        side. One of the reasons is that the live feed continues
        independent of the client side ad insertion. In that case the
        player can easily fall behind the live window. Server side ad
        insertions allow the player to play a continuous feed, independent
        of any ad insertions, which are "injected" on the server side,
        directly into the playout feed.
        </p>
        <p>
        One of the requirements of the underlying streaming format such as
        DASH or HLS is that the encoding does not change during the playout
        of a single rendition. To still be able to insert advertisements,
        HLS and DASH propose different strategies.
        </p>
        <p>
        HLS allows the packager to add a "discontinuity" tag. This is
        expressed with a <code>#EXT-X-DISCONTINUITY</code> entry before any
        potential format change.  See Section 4.3.2.3. in [[HLS]] for more
        detail. This tag then usually appears before the ad starts and
        again before the main content continues.
        </p>
        <p>
        In contrast, DASH uses multiple "periods" to split the content into
        main and advertisement sections.  See Section 3.1.31 in [[MPEGDASH]]
        for more details. Each switch between periods is a trigger for the
        player to potentially reset the decoder.
        </p>
        <p>
        Where the player usually receives dedicated events in CSAI such as
        'adStarted' or 'adEnded', Server-side insertions require a
        different setup to trigger such events.  Typically, in-band
        notification are used to trigger time based events. For example,
        SCTE-35 markers are a common format to insert time based
        meta-data into the feed. These markers are read and interpreted
        by the player and can be used to trigger events or carry additional
        meta-data such as callback URLs.
        </p>
        </section>
      </section>
    </section>
      <section>
      <h2>Content Encoding Guidelines</h2>
      <p>Encoding video for playback can be a challenge given the variety of ways users may watch your content.  In order to provide quality video for your audience, there are multiple aspects to consider such as codec support, resolutions, frame-rates,  browser versions, bandwidth availability and device compatibilities.  Fortunately there are some steps you can follow that will help you produce content that is playable for all audiences.  Preparing your source content and planning your outputs ahead of time are important activities to complete prior to actual transcoding.  Time spent here will help reduce trial and error later, and ensures maximum quality throughout your workflow, resulting in an excellent viewer experience for your entire audience.  If these steps are rushed, then there will certainly be inefficiencies in video processing and wasted bandwidth, and the outputs may not be sufficient to cover your target audience, leading to higher cost and missed opportunities.  There are a few key steps to get your content ready for web delivery.</p>

      <p>These steps are best done in the following order :</p>

      <ol>
        <li>Create a mezzanine file</li>
        <li>Decide on rendition set</li>
        <li>Decide on delivery formats to support</li>
        <li>Create encoding profiles</li>
        <li>Transcode mezzanine into specific rendition output formats</li>
      </ol>

      <section>
      <h4>Create a Mezzanine File</h4>

      <p>The first step in preparing your content is to create a high quality encoding master file from your source footage. This mezzanine file will be used as a source file to create all downstream outputs.  Content producers typically export files from a non-linear editor using the highest resolution input files available; from a master magnetic, optical media or digital source.  This ensures all downstream outputs have the necessary quality to work with, without needing to re-export from the editor every time. </p>

      <p>In order to maximize quality of the mezzanine, your export settings should use a lossless codec such as Apple ProRes, H.264 Intra-frame, or Motion JPEG 2000.  A lossless codec guarantees all data from the original will exist on the export. For detailed info on how to configure your output settings, see the appropriate provider recommended settings documentation for your codec of choice.</p>

      <p>Mezzanines are typically rendered in the native
        resolution and frame-rate that the source material was captured with. A common
        configuration using Apple ProRes would be as follows:</p>
      <p>
        <table border="1" cellspacing="0" cellpadding="4">
          <colgroup><col width="181"><col width="181"><col width="181"><col width="384"></colgroup>
          <tbody>
            <tr>
              <td>Codec</td>
              <td>Resolution</td>
              <td>Bitrate</td>
              <td>Framerates</td>
            </tr>
            <tr>
              <td rowspan="4">Apple ProRes 422</td>
              <td>3840x2160</td>
              <td>340-1650 Mbps</td>
              <td>23.98, 24, 25, 29.97, 30, 50, 59.94, 60</td>
            </tr>
            <tr>
              <td>1920x1080</td>
              <td>145-220 Mbps</td>
              <td>23.98, 24, 25, 29.97, 30, 50, 59.94, 60</td>
            </tr>
            <tr>
              <td>1280x720</td>
              <td>20-60 Mbps</td>
              <td>23.98, 24, 25, 29.97, 30, 50, 59.94, 60</td>
            </tr>
            <tr>
              <td>640x480</td>
              <td>8-12 Mbps</td>
              <td>25, 29.97, 50, 59.94</td>
            </tr>
          </tbody>
        </table>
      </p>
      </section>
      <section>
      <h4>Decide on Rendition Set</h4>
      <p>In modern video streaming use-cases, adaptive bitrate (ABR) streaming technologies use a rendition set to ensure every playback environment has an appropriate file to render.  A rendition set is simply a grouping of the different transcodes of the same mezzanine file. During playback,  ABR technologies detect the user's playback environment, available codecs, resolution and bandwidth; then selects the right segment from one of the video renditions to send to the video player.  This reduces probability of buffering as only the segments of the matching video rendition is loaded.  To create a proper rendition set, you should list the codecs and resolutions that exists for your target audience. Then you need to ascertain the proper bitrate for each codec &amp; resolution, high enough to meet your quality goals but not exceed bandwidth availability.<p>

      <p>In order to select optimum bitrates, determine your target user based upon
          target devices and user experience. For example, for OTT (Over-the-top) 
        applications for Smart Televisions, typically a higher bitrate rendition is 
        set as the default since resolution &amp; bandwidth are reliably available.  The table 
          below represents a rendition set for an OTT delivery that uses an ABR technology</p>

      <p>
        <table border="1" cellspacing="0" cellpadding="4">
          <colgroup><col width="181"><col width="181"><col width="181"><col width="384"></colgroup>
          <tbody>
            <tr>
              <td>Codec</td>
              <td>Resolution</td>
              <td>Bitrate</td>
              <td>Framerates</td>
            </tr>
            <tr>
              <td rowspan="4">HEVC</td>
              <td>2160p</td>
              <td>10 Mbps</td>
              <td>23.98, 24, 25, 29.97, 30, 50, 59.94, 60</td>
            </tr>
            <tr>
              <td>1080p</td>
              <td>5 Mbps<br>
                3 Mbps</td>
              <td>23.98, 24, 25, 29.97, 30, 50, 59.94, 60</td>
            </tr>
            <tr>
              <td>720p</td>
              <td>2 Mbps<br>
                1 Mbps</td>
              <td>23.98, 24, 25, 29.97, 30, 50, 59.94, 60</td>
            </tr>
            <tr>
              <td>480p</td>
              <td>-</td>
              <td>Typically not done for HEVC because of resource cost</td>
            </tr>
            <tr>
              <td rowspan="4">H.264</td>
              <td>2160p</td>
              <td>18 Mbps</td>
              <td>23.98, 24, 25, 29.97, 30, 50, 59.94, 60</td>
            </tr>
            <tr>
              <td>1080p</td>
              <td>9 Mbps<br>
                5 Mbps</td>
              <td>23.98, 24, 25, 29.97, 30, 50, 59.94, 60</td>
            </tr>
            <tr>
              <td>720p</td>
              <td>4 Mbps<br>
                2.5 Mbps</td>
              <td>23.98, 24, 25, 29.97, 30, 50, 59.94, 60</td>
            </tr>
            <tr>
              <td>480p</td>
              <td>1.5 Kbps<br>
                768 Kbps<br>
                380 Kbps</td>
              <td>25, 29.97, 50, 59.94</td>
            </tr>
          </tbody>
        </table>
        </section>
        <section>
		<h4>Decide on Delivery Formats to support</h4>
      	<p>In addition to defining resolutions and bitrates in your Rendition Set, the delivery format should also be considered.  For most audiences, a format of a MP4 container with H.264 video and AAC audio is sufficient for most SD/HD videos.  However if your video content warrants the need for advanced features, then you should also consider adding a renditions to your set that includes higher profile H.264, H.265, VP9, AAC-HI formats, so that users with environments supporting those features can enjoy them, while still providing a baseline experience via the H.264/AAC MP4 rendition set.</p>
      	</section>
      	<section>
		<h4>Create encoding profiles</h4>
      	<p>After you have listed all the renditions that will be necessary, then create transcoding profiles (or templates depending on your tool) for each rendition in your transcoding software of choice (FFMPEG, Compressor, Vantage). </p>
      	</section>
      	<section>
		<h4>Transcode</h4>
      	<p>If all the previous steps have been carefully thought out, then actual transcoding is comparatively easy.  It is the act of taking the mezzanine file and sending it to each transcoding profile.  To save time, this should be eventually scripted if possible since you may need to revisit this step each time you adjust your rendition set.</p>

		<p>At the end of this process you should have a set of files that can be hosted at for web delivery.  You will need to package these so that your chosen ABR technology can utilize them, this process is known as packaging.</p>
		</section>


		<section>
      	<h4>Packaging your video files for ABR delivery</h4>
		<p>In order for an ABR technology to utilize your rendition set, each file needs to be properly segmented and exposed to the ABR technology following their spec.  Segmentation refers to the breakdown of each file into timed chunks so ABR can smoothly switch renditions without causing a break in the video playback.  Exposure of rendition set and segments are done through a manifest file - .m3u8 for HLS and .mpd for DASH.  The manifest is simply a text file that serves as a directory for the renditions and includes details (resolution, codec, bitrate, etc) about each file available.  This is necessary so that the ABR can find the correct segment for the users playback environment. A manifest can also contain references to other files that may be necessary for specific video features, such as .vtt files for subtitles and additional audio tracks for multi-lingual video. </p>
		<p>Information about authoring a manifest for HLS can be found here, https://tools.ietf.org/html/draft-pantos-http-live-streaming-23 and authoring for DASH can be here, http://dashif.org/guidelines/ </p>
		</section>
    </section>
<section>





</section>
    <section>
      <h2>Web App Structure</h2>
      <p>
        Media companies more and more are making their content and channels available where ever users are consuming media.  More often than not this includes the web and native app platforms.  As we see rise in media companies using web technologies to build cross platform apps (that stretch across the web and native app platforms) it becomes more important to define clarify the different approaches you can take to building "apps" with web technologies.
      </p>
      <section>
        <h3>Web App Content Management Approaches</h3>
        <p>The easiest way to delineate between the types of Web apps are by how we manage the content inside them. In the case of traditional applications and mobile apps, the entire application is downloaded from the store and then ran locally. In contrast, a website keeps all of its content on the server and then when the pages loader. It brings the entire application down into the browser. The same approaches are utilized with web content inside of the app space there often known as packaged web apps and hosted web apps.
        </p>
        <section>
          <h4>The Packaged App</h4>
          <p>The Packaged App is a type of web app that distributed in its entirety to a user through a single download. Generally, we see packaged apps within App Markets. Packaged Apps have these characters:</p>
          <ul>
            <li>Downloaded in entirety through a store or market</li>
            <li>Generally offline by default, as all the code is living on the users device</li>
            <li>Retrieve data from the Internet via services similar to native apps. generally called via AJAX</li>
          </ul>
          <p>Some Markets, such as Window Store and Amazon App Store enable you to submit packaged apps directly to the market without the need of any external packaging tools. These markets also expose additional APIs for packaged apps that aren’t available to apps via the browser. Other markets, such as iOS App Store and Android Play require a third party utility such as Cordova or cross walk to enable packaged web apps.
          </p>
        </section>
      <section>
        <h4>The Hosted App</h4>
        <p>Hosted Apps are web apps submitted to app markets that point to web content that remains on the webserver as appose to being packaged and submitted to the store. Hosted app’s have these Characteristics:
        </p>
        <ul>
          <li>Have a thin layer of native code, usually containing a webview and / or an app manifest that is submitted to the market</li>
          <li>Can run dynamic web content from web server (thank asp.net PHP, etc).</li>
          <li>By default have no functionality when device is offline (just like web)</li>
          <li>Hosted web apps are updated on the web server instead of pushing packages to the app store.</li>
        </ul>
      </section>
      <section>
        <h4>Progressive Web Apps</h4>
        <p>
          Progressive Web Apps or PWAs has become the de facto app format for the web. Just like hosted web apps, Progressive Web Apps load their content from the server when the app is opened. However, Progressive Web Apps also can store this web app locally through a new technology called service workers, which allows Progressive Web Apps to bridge across both the benefits of package web apps and hosted web apps.  Some platforms run PWAs in the browser while others (like android and Windows 10) run PWAs in a stand along app container.
        </p>
        <p>
          PWAs are build around a common universal <a href="http://www.w3.org/TR/appmanifest/">web app manifest</a> that is being standardized by the W3C.  The web manifest contains meta data about your application such as presentation preference, start URL, and even ratings and categories for store listing.
        </p>
        <pre class="example json" title="common manifest">
          {
            "lang": "en",
            "dir": "ltr",
            "name": "Donate App",
            "description": "This app helps you donate to worthy causes.",
            "short_name": "Donate",
            "icons": [{
              "src": "icon/lowres.webp",
              "sizes": "64x64",
              "type": "image/webp"
            },{
              "src": "icon/lowres.png",
              "sizes": "64x64"
            }, {
              "src": "icon/hd_hi",
              "sizes": "128x128"
            }],
            "scope": "/racer/",
            "start_url": "/racer/start.html",
            "display": "fullscreen",
            "orientation": "landscape",
            "theme_color": "aliceblue",
            "background_color": "red",
            "serviceworker": {
              "src": "sw.js",
              "scope": "/racer/",
              "use_cache": false
            },
            "screenshots": [{
              "src": "screenshots/in-game-1x.jpg",
              "sizes": "640x480",
              "type": "image/jpeg"
            },{
              "src": "screenshots/in-game-2x.jpg",
              "sizes": "1280x920",
              "type": "image/jpeg"
            }]
          }

        </pre>
        <p>
          You can learn more about building Progressive Web Apps <a href="https://developers.google.com/web/progressive-web-apps/">here</a>.
        </p>
      </section>
      </section>
      <section>
        <h3>Web Based Platforms</h3>
        <p>
          Some platforms such as web OS are themselves a web based environment, meaning all applications in that environment or built with the web. And run as independent applications within the environment just like a web page may run inside of a web browser.  Device manufacturers (of all kinds) use web engines as a basis for a web based platform that will allow web apps to run as standalone applications.
        </p>
        <p>
          The most common platform may be <a href="http://www.chromium.org/chromium-os">ChromeOS</a> itself.  ChromeOS is Chromium running on top of Linux that uses Web Apps for it's primary app Eco system (however it recently has been augmented to run android apps as well).  Another common Example is WebOS, which powers many LG tvs today, and runs web content as stand alone apps.
        </p>
      </section>
      <section>
        <h3>Web App Containers</h3>
        <p>
          Delivering the Web app on the platform is not a trivial task as each platform may have a different approach on how a web app is delivered to a user. Some platforms such as <a href="https://developer.microsoft.com/en-us/windows/bridges/hosted-web-apps">Xbox (windows 10)</a> and <a href="https://www.playstation.com/develop/">Play station</a> give the Web apps their own standalone container, which allows the app to be downloaded from a store and run independently of a browser.  These apps are rendered with the web rendering engine native to the operating system, and generally have lower overhead than running in a browser, and often are granted access to OS level APIs not available in the browser.
        </p>
        <section>
          <h4>Mobile App Containers</h4>
          <p>
            For a Web app to be distributed through the store on platforms like iOS and Android, an app container needs to be used. popular app containers such as <a href="http://cordova.apache.org/">Cordova</a> and <a href="https://crosswalk-project.org/">cross walk</a> are used by web developers to deliver a web app through a store.  This approach often adds over head to the application as the web app is rendered in a webview or the app may contain the entire browser runtime within each app container.
          </p>
        </section>
        <section>
          <h4>Chrome Embedded Framework and Electron</h4>
          <p>
            In desktop environments, we see the use of <a href="https://electron.atom.io/">Electron</a> and <a href="https://code.google.com/archive/p/chromiumembedded/">Chromium Embedded Framework (CEF)</a> to deliver web apps as desktop applications.  CEF and Electron uses the open source version of chrome as a runtime environment for the application. Much like we see one mobile platforms, this approach has additional overhead as instead of using the native rendering engine of the platform, chromium is delivered inside each application to render the web app. This gives the Web app additional privileges to access native API’s, but at the same time raises many security concerns.
          </p>
        </section>
      </section>
    </section>
  </body>
</html>
